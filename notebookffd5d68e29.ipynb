{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d361ab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-23T15:21:36.848800Z",
     "iopub.status.busy": "2025-01-23T15:21:36.848301Z",
     "iopub.status.idle": "2025-01-23T15:21:37.844101Z",
     "shell.execute_reply": "2025-01-23T15:21:37.842732Z"
    },
    "papermill": {
     "duration": 1.00226,
     "end_time": "2025-01-23T15:21:37.846302",
     "exception": false,
     "start_time": "2025-01-23T15:21:36.844042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/Predict-the-Customer-Satisfaction-CSE-22/sample_submission.csv\n",
      "/kaggle/input/Predict-the-Customer-Satisfaction-CSE-22/train_dataset.csv\n",
      "/kaggle/input/Predict-the-Customer-Satisfaction-CSE-22/test_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63175b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T15:21:37.852823Z",
     "iopub.status.busy": "2025-01-23T15:21:37.852334Z",
     "iopub.status.idle": "2025-01-23T15:22:59.628998Z",
     "shell.execute_reply": "2025-01-23T15:22:59.627783Z"
    },
    "papermill": {
     "duration": 81.782428,
     "end_time": "2025-01-23T15:22:59.631359",
     "exception": false,
     "start_time": "2025-01-23T15:21:37.848931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('/kaggle/input/Predict-the-Customer-Satisfaction-CSE-22/train_dataset.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/Predict-the-Customer-Satisfaction-CSE-22/test_dataset.csv')\n",
    "\n",
    "# Handling missing values using recommended assignment approach\n",
    "train_df['loyalty_tier'] = train_df['loyalty_tier'].fillna(train_df['loyalty_tier'].mode()[0])\n",
    "test_df['loyalty_tier'] = test_df['loyalty_tier'].fillna(test_df['loyalty_tier'].mode()[0])\n",
    "\n",
    "train_df['Received_tier_discount_percentage'] = train_df['Received_tier_discount_percentage'].fillna(\n",
    "    train_df['Received_tier_discount_percentage'].median()\n",
    ")\n",
    "test_df['Received_tier_discount_percentage'] = test_df['Received_tier_discount_percentage'].fillna(\n",
    "    test_df['Received_tier_discount_percentage'].median()\n",
    ")\n",
    "\n",
    "train_df['Received_card_discount_percentage'] = train_df['Received_card_discount_percentage'].fillna(\n",
    "    train_df['Received_card_discount_percentage'].median()\n",
    ")\n",
    "test_df['Received_card_discount_percentage'] = test_df['Received_card_discount_percentage'].fillna(\n",
    "    test_df['Received_card_discount_percentage'].median()\n",
    ")\n",
    "\n",
    "# Encode categorical variables\n",
    "# Encode Gender column (M: 1, F: 0, O: 2)\n",
    "train_df['Gender'] = train_df['Gender'].map({'M': 1, 'F': 0, 'O': 2})\n",
    "test_df['Gender'] = test_df['Gender'].map({'M': 1, 'F': 0, 'O': 2})\n",
    "\n",
    "# Encode loyalty program membership (YES: 1, NO: 0)\n",
    "train_df['Is_current_loyalty_program_member'] = train_df['Is_current_loyalty_program_member'].map({'YES': 1, 'NO': 0})\n",
    "test_df['Is_current_loyalty_program_member'] = test_df['Is_current_loyalty_program_member'].map({'YES': 1, 'NO': 0})\n",
    "\n",
    "# Encode target variable 'customer_experience'\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['customer_experience'] = label_encoder.fit_transform(train_df['customer_experience'])\n",
    "\n",
    "# Convert date columns to datetime and extract year, month, day\n",
    "date_columns = ['Date_Registered', 'payment_datetime', 'purchased_datetime', \n",
    "                'released_date', 'estimated_delivery_date', 'received_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    train_df[col] = pd.to_datetime(train_df[col], errors='coerce')\n",
    "    test_df[col] = pd.to_datetime(test_df[col], errors='coerce')\n",
    "    \n",
    "    train_df[f'{col}_year'] = train_df[col].dt.year\n",
    "    train_df[f'{col}_month'] = train_df[col].dt.month\n",
    "    train_df[f'{col}_day'] = train_df[col].dt.day\n",
    "\n",
    "    test_df[f'{col}_year'] = test_df[col].dt.year\n",
    "    test_df[f'{col}_month'] = test_df[col].dt.month\n",
    "    test_df[f'{col}_day'] = test_df[col].dt.day\n",
    "\n",
    "# Drop original date columns\n",
    "train_df.drop(columns=date_columns, inplace=True)\n",
    "test_df.drop(columns=date_columns, inplace=True)\n",
    "\n",
    "# Standardize the 'final_payment' column\n",
    "scaler = StandardScaler()\n",
    "train_df[['final_payment']] = scaler.fit_transform(train_df[['final_payment']])\n",
    "test_df[['final_payment']] = scaler.transform(test_df[['final_payment']])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['user_id', 'transaction_id', 'order_id', 'tracking_number']\n",
    "train_df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "test_df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_columns = ['product_category', 'payment_method', 'shipping_method', 'purchase_medium']\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_columns, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Align test dataset columns with train dataset\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n",
    "\n",
    "# Fill any remaining missing values with 0\n",
    "train_df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)\n",
    "\n",
    "# Split features and target\n",
    "train_x = train_df.drop(columns=['customer_experience'])  # Features\n",
    "train_y = train_df['customer_experience']  # Target variable\n",
    "\n",
    "# Ensure test set doesn't contain the target column\n",
    "test_x = test_df.drop(columns=['customer_experience'], errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Train Random Forest classifier model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1a7403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T15:22:59.638492Z",
     "iopub.status.busy": "2025-01-23T15:22:59.638055Z",
     "iopub.status.idle": "2025-01-23T15:22:59.645379Z",
     "shell.execute_reply": "2025-01-23T15:22:59.644220Z"
    },
    "papermill": {
     "duration": 0.012771,
     "end_time": "2025-01-23T15:22:59.647228",
     "exception": false,
     "start_time": "2025-01-23T15:22:59.634457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0s: 56588\n",
      "Count of 1s: 72938\n",
      "Count of 2s: 8445\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each label\n",
    "count_zeros = np.count_nonzero(predictions == 0)\n",
    "count_ones = np.count_nonzero(predictions == 1)\n",
    "count_twos = np.count_nonzero(predictions == 2)\n",
    "\n",
    "print(f\"Count of 0s: {count_zeros}\")  \n",
    "print(f\"Count of 1s: {count_ones}\")   \n",
    "print(f\"Count of 2s: {count_twos}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2070c212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T15:22:59.653436Z",
     "iopub.status.busy": "2025-01-23T15:22:59.652939Z",
     "iopub.status.idle": "2025-01-23T15:22:59.769897Z",
     "shell.execute_reply": "2025-01-23T15:22:59.768499Z"
    },
    "papermill": {
     "duration": 0.122143,
     "end_time": "2025-01-23T15:22:59.771828",
     "exception": false,
     "start_time": "2025-01-23T15:22:59.649685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Convert predictions back to original labels for interpretation\n",
    "decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "# Export predictions if needed\n",
    "output = pd.DataFrame({ 'id': range(len(predictions)),\n",
    "    'customer_experience': decoded_predictions})\n",
    "# Save the DataFrame to a CSV file with the specified format\n",
    "output.to_csv('customer_satisfaction_predictions.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7360c0",
   "metadata": {
    "papermill": {
     "duration": 0.002084,
     "end_time": "2025-01-23T15:22:59.776532",
     "exception": false,
     "start_time": "2025-01-23T15:22:59.774448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10664448,
     "sourceId": 90922,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 86.77284,
   "end_time": "2025-01-23T15:23:00.700941",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-23T15:21:33.928101",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
